name: MLOps Lab CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      deploy_environment:
        description: 'Deploy to environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production

env:
  DOCKER_IMAGE_NAME: mlops-lab
  PYTHON_VERSION: '3.9'

jobs:
  # Lint and Code Quality
  lint:
    name: Lint and Code Quality
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Check code formatting with black
      run: black --check --diff .

    - name: Check import sorting with isort
      run: isort --check-only --diff .

  # Unit Tests
  test:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install coverage
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Run unit tests with coverage
      run: |
        coverage run -m unittest discover -s . -p "test_*.py" -v
        coverage report -m
        coverage xml

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # Docker Build and Test
  docker-test:
    name: Docker Build and Test
    runs-on: ubuntu-latest
    needs: test
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image for testing
      run: |
        docker build --target test -t ${{ env.DOCKER_IMAGE_NAME }}:test .

    - name: Run tests in Docker
      run: |
        docker run --rm ${{ env.DOCKER_IMAGE_NAME }}:test

    - name: Build Docker image for training
      run: |
        docker build --target train -t ${{ env.DOCKER_IMAGE_NAME }}:train .

    - name: Build Docker image for build stage
      run: |
        docker build --target build -t ${{ env.DOCKER_IMAGE_NAME }}:build .

    - name: Build Docker image for deployment
      run: |
        docker build --target deploy -t ${{ env.DOCKER_IMAGE_NAME }}:deploy .

    - name: Test deployment image
      run: |
        docker run --rm -d --name test-deploy ${{ env.DOCKER_IMAGE_NAME }}:deploy
        sleep 5
        docker logs test-deploy
        docker stop test-deploy

  # Security Scanning
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: lint
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Run safety check
      run: safety check

    - name: Run bandit security scan
      run: bandit -r . -f json -o bandit-report.json || true

    - name: Upload bandit report
      uses: actions/upload-artifact@v3
      with:
        name: bandit-report
        path: bandit-report.json

  # Build and Push Docker Image
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, docker-test, security]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix=commit-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        target: deploy
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  # Deploy to Staging
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment: staging
    steps:
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        echo "Image: ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE_NAME }}:latest"
        # Add your staging deployment commands here
        # For example: kubectl, docker-compose, or cloud provider CLI commands

  # Deploy to Production
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: deploy-staging
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.deploy_environment == 'production'
    environment: production
    steps:
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        echo "Image: ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE_NAME }}:latest"
        # Add your production deployment commands here

  # Performance Testing
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install performance testing tools
      run: |
        python -m pip install --upgrade pip
        pip install pytest-benchmark
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

    - name: Run performance benchmarks
      run: |
        # Create a simple benchmark test
        cat > test_performance.py << EOF
        import pytest
        from factorial import factorial
        from prime import is_prime
        from palindrome import is_palindrome
        from fibo import fibonacci_sequence
        from ascend import sort_ascending
        
        def test_factorial_performance(benchmark):
            result = benchmark(factorial, 20)
            assert result > 0
        
        def test_prime_performance(benchmark):
            result = benchmark(is_prime, 97)
            assert result == True
        
        def test_palindrome_performance(benchmark):
            result = benchmark(is_palindrome, "A man a plan a canal Panama")
            assert result == True
        
        def test_fibonacci_performance(benchmark):
            result = benchmark(fibonacci_sequence, 100)
            assert len(result) > 0
        
        def test_sort_performance(benchmark):
            data = [i for i in range(100, 0, -1)]
            result = benchmark(sort_ascending, data)
            assert result == sorted(data)
        EOF
        
        pytest test_performance.py --benchmark-only --benchmark-json=benchmark.json

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: benchmark.json

  # Notification
  notify:
    name: Notify
    runs-on: ubuntu-latest
    needs: [deploy-staging, performance-test]
    if: always()
    steps:
    - name: Notify success
      if: needs.deploy-staging.result == 'success'
      run: |
        echo "✅ Pipeline completed successfully!"
        echo "- Tests passed"
        echo "- Security scan completed"
        echo "- Docker image built and pushed"
        echo "- Deployed to staging"

    - name: Notify failure
      if: failure()
      run: |
        echo "❌ Pipeline failed!"
        echo "Check the logs for more details."